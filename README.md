# Image Text VQA Model - Multimodal Mini Project

A lightweight Visual Question Answering (VQA) system that combines computer vision and natural language processing to answer questions based on input images. This project integrates **CLIP** for image feature extraction and a **small text model (DistilGPT2)** for text generation.

## ðŸ“Œ Project Overview

This project implements a generative VQA architecture designed for efficiency and semantic accuracy. It was built as a self-directed mini-project to explore multimodal learning and optimization techniques.

### Key Features
* **Multimodal Architecture**: Fuses **OpenAI CLIP** (Vision Transformer) embeddings with **DistilGPT2**.
* **Mixed Precision Training**: Implements `torch.amp` (Automatic Mixed Precision) to optimize memory usage and speed up training by **2x** on compatible hardware.
* **Robust Evaluation**: Uses **BERTScore** to evaluate the semantic similarity between generated answers and ground truth, providing a better metric than standard accuracy or BLEU scores.
* **Hyperparameter Tuning**: optimized for performance on consumer-grade GPUs/CPUs.

## ðŸ“‚ Project Structure

```text
â”œâ”€â”€ config.py           # Central configuration for hyperparameters and paths
â”œâ”€â”€ dataset.py          # Custom PyTorch Dataset class for loading images and text
â”œâ”€â”€ model.py            # Neural network architecture (CLIP + Projection + GPT2)
â”œâ”€â”€ train.py            # Training loop with Mixed Precision support
â”œâ”€â”€ evaluate.py         # Inference script using BERTScore for metrics
â”œâ”€â”€ utils.py            # Helper functions for text cleaning and formatting
â”œâ”€â”€ requirements.txt    # List of dependencies
â””â”€â”€ data/               # Directory for dataset storage
    â”œâ”€â”€ images/         # Folder containing image files
    â””â”€â”€ vqa_dataset.csv # CSV file with image_id, questions, and answers
